# LLM Paper Reading
This repo lists some interesting LLM related papers.  

## LLM Embedding
* [Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://arxiv.org/pdf/2310.08319.pdf)
  * Keywords: Dense Retriever(RepLLaMA), Pointwise Reranker(RankLLaMA), Contrastive loss, MS MARCO, BEIR
* [Improving Text Embeddings with Large Language Models](https://arxiv.org/pdf/2401.00368.pdf)
  * Keywords: Synthetic Data Generation, Mistral-7b, Contrastive loss, Multilingual Retrieval, BEIR, MTEB

## LLM Prompting


## LLM Training
* [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)
  * Keywords: Low Rank Adapters (LoRA), 4-bit NormalFloat (NF4), Double Quantization, Paged Optimizers

## LLM Inference
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf)
  * Keywords: Self-attention, IO-aware,  GPU high bandwidth memory (HBM), GPU SRAM, Block-Sparse Attention
* [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180.pdf)
  * Keywords: KV cache, PagedAttention, Classical Virtual Memory, Paging techniques, vLLM


## LLM Architectures
* [Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)
  * Keywords:  Mixtral 8x7B, Sparse Mixture of Experts (SMoE)
* [Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)
  * Keywords:  Grouped-Query Attention (GQA), Sliding Window Attention (SWA), Rolling Buffer Cache, Pre-fill and Chunking
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)
  * Keywords: Grouped-Query Attention (GQA), Context Length 4k, 2.0T Tokens
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
  * Keywords: Pre-normalization, SwiGLU activation function, Rotary Positional Embeddings (RoPE), Context Length 2k, 1.0T Tokens


## LLM Agents

## LLM Alignment
* [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267.pdf)
  * Keywords: RL from AI Feedback (RLAIF), Generating AI labels, Self Improvement

## LLM Survey
* [A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/pdf/2401.01313.pdf)
  * Keywords: LLM Hallucination, RAG, Knowledge Retrieval, CoNLI, CoVe
* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)
  * Keywords:  PLMs, LLMs, Pre-training, Adaptation tuning, Capacity evaluation


## Blogs
* RAGAS A framework to evaluate RAG: https://docs.ragas.io/en/stable/


